{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crk7-YfvTvj-"
   },
   "source": [
    "# Setup and imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup pointcloud libraries"
   ],
   "metadata": {
    "id": "HeOl7LuWHNW7"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "h-Ir_i2D9ETg"
   },
   "source": [
    "import os\n",
    "!pip install trimesh pyvista\n",
    "!pip install torch_geometric\n",
    "!pip install huggingface_hub\n",
    "!pip install pyg_lib torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch.nn as nn\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "\n",
    "torch.__version__"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kzE37ZiHgQwW",
    "outputId": "25a6ddd8-d594-4593-9c80-b4886bdbd5be"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup pre-made AB-UPT components\n",
    "These will be used later on in the tutorial."
   ],
   "metadata": {
    "id": "CUyuDakRG8JT"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "RIxtwfPIR6Q8"
   },
   "source": [
    "!unzip anchored-branched-universal-physics-transformers-main.zip -d ."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXbDhKdySq80",
    "outputId": "93d2aa7b-b42d-4bd7-8465-756fc6f84258"
   },
   "source": [
    "%cd anchored-branched-universal-physics-transformers-main/src"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lioFQO6kNXUS"
   },
   "source": [
    "from abupt_collator import AbuptCollator\n",
    "from drivaerml_dataset import DrivAerMLDataset\n",
    "from model import AnchoredBranchedUPT\n",
    "from preprocessors import MomentNormalizationPreprocessor, PositionNormalizationPreprocessor\n",
    "from streamline_visualization import plot_streamlines\n",
    "from utils import plot_pointcloud_single, plot_pointcloud_double, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tmCzzi31PD2"
   },
   "source": [
    "# Data download\n",
    "\n",
    "First, we need to download the data which is hosted on [huggingface](https://huggingface.co/datasets/neashton/drivaerml).\n",
    "\n",
    "We will only consider run_11, which is a design that we put into the testset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FeaZQ3ryx0wv"
   },
   "source": [
    "sh = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Set the path and prefix\n",
    "HF_OWNER=\"neashton\"\n",
    "HF_PREFIX=\"drivaerml\"\n",
    "\n",
    "# Set the local directory to download the files\n",
    "LOCAL_DIR=\"../data\"\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "mkdir -p \"$LOCAL_DIR\"\n",
    "\n",
    "# Loop through the run folders from 1 to 500\n",
    "for i in $(seq 11 11); do\n",
    "    RUN_DIR=\"run_$i\"\n",
    "    RUN_LOCAL_DIR=\"$LOCAL_DIR/$RUN_DIR\"\n",
    "\n",
    "    # Create the run directory if it doesn't exist\n",
    "    mkdir -p \"$RUN_LOCAL_DIR\"\n",
    "\n",
    "    # Download drivaer_i.stl (geometry file) and boundary_i.stl (surface variables)\n",
    "    wget \"https://huggingface.co/datasets/${HF_OWNER}/${HF_PREFIX}/resolve/main/$RUN_DIR/drivaer_$i.stl\" -O \"$RUN_LOCAL_DIR/drivaer_$i.stl\"\n",
    "    wget \"https://huggingface.co/datasets/${HF_OWNER}/${HF_PREFIX}/resolve/main/$RUN_DIR/boundary_$i.vtp\" -O \"$RUN_LOCAL_DIR/boundary_$i.vtp\"\n",
    "    wget \"https://huggingface.co/datasets/${HF_OWNER}/${HF_PREFIX}/resolve/main/$RUN_DIR/geo_ref_$i.csv\" -O \"$RUN_LOCAL_DIR/geo_ref_$i.csv\"\n",
    "\n",
    "\n",
    "done\n",
    "\"\"\"\n",
    "with open('download.sh', 'w') as file:\n",
    "    file.write(sh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OXK1S1MlzSGL"
   },
   "source": [
    "!bash download.sh"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt-bKf1rDO3X"
   },
   "source": [
    "# Visualize geometry"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8NpqIVHJEGjm"
   },
   "source": [
    "# load geometry and surface data (from simulation) into memory\n",
    "stl = trimesh.load(\"../data/run_11/drivaer_11.stl\", force=\"mesh\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tegZxT42A5aJ"
   },
   "source": [
    "# interactive visualization of the STL geometry\n",
    "#stl.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# static image of the geometry\n",
    "img = mpimg.imread('../images/visualization_geometry_static.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "_p6GE5quizZU",
    "outputId": "361730c8-aee2-4675-8624-282e27cf55bd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4017-okZDYeZ"
   },
   "source": [
    "# Inspect data\n",
    "\n",
    "Now that we know what we are working with, we can have a more detailed look at what we extract from the dataset and what shapes these items have. We also convert it to torch tensors such that we can easily load them afterwards with our `DrivaermlDataset` implementation in `src/drivaerml_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5ZYI_3sX7QX"
   },
   "source": [
    "## Surface-level\n",
    "\n",
    "Surface-level data can be directly extracted from the `.vtp` file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "edpxnUI2A5zm"
   },
   "source": [
    "vtp = pv.read(\"../data/run_11/boundary_11.vtp\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "dxTlDaONF8zS",
    "outputId": "35886bbf-3258-496d-bce8-3f9d44ede6e0"
   },
   "source": [
    "surface_position = torch.from_numpy(vtp.cell_centers().points)\n",
    "print(f\"{len(surface_position) / 1000 / 1000:.1f}M points in 3D space\")\n",
    "print(f\"surface_position.shape: {surface_position.shape}\")\n",
    "torch.save(surface_position, \"../data/run_11/surface_position_vtp.pt\")\n",
    "plot_pointcloud_single(\n",
    "    surface_position,\n",
    "    # increase this for higher resolution or larger plot\n",
    "    num_points=10000,\n",
    "    figsize=(6, 6)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "CY0qhHazF_HU",
    "outputId": "24340098-1d39-46c1-ba5b-1869ecbc5ac9"
   },
   "source": [
    "surface_pressure = torch.from_numpy(vtp.get_array(\"pMeanTrim\"))\n",
    "print(f\"{len(surface_position) / 1000 / 1000:.1f}M points with a scalar pressure value\")\n",
    "print(f\"surface_pressure.shape: {surface_pressure.shape}\")\n",
    "torch.save(surface_pressure, \"../data/run_11/surface_pressure.pt\")\n",
    "plot_pointcloud_single(\n",
    "    surface_position,\n",
    "    # adjust clamp values for different color scale (original range is roughly -2500 to 700)\n",
    "    color=surface_pressure.clamp(-1500, 500),\n",
    "    title=\"surface pressure\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=20000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "DrfQhZKrGN43",
    "outputId": "5ff8f0f9-b602-4217-a415-ef1d16885033"
   },
   "source": [
    "wallshearstress = torch.from_numpy(vtp.get_array(\"wallShearStressMeanTrim\"))\n",
    "print(f\"{len(surface_position) / 1000 / 1000:.1f}M points with a 3D wallshearstress vector\")\n",
    "print(f\"wallshearstress.shape: {wallshearstress.shape}\")\n",
    "torch.save(wallshearstress, \"../data/run_11/surface_wallshearstress.pt\")\n",
    "plot_pointcloud_single(\n",
    "    surface_position,\n",
    "    # adjust clamp values for different color scale (original range is roughly 0 to 30)\n",
    "    color=wallshearstress.norm(dim=1).clamp(0, 5),\n",
    "    title=\"wallshearstress\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=20000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeKaGJLMYO0x"
   },
   "source": [
    "## Volume-level data\n",
    "Volume-level data is quite large, so downloading and storing the full data is not feasible in colab. We therefore provide already preprocessed (1000x subsampled) volumetric data.\n",
    "\n",
    "Note that 134K volume cells loaded here would be 134M in the raw data.\n",
    "\n",
    "Also note that the CFD mesh highly resolves regions close to the surface, which makes the volume visualization look like a \"silhouette\" of the car."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "i6ahyn1ZYfjj",
    "outputId": "fd038517-10c0-45e5-bdc0-f12eeec9a1d8"
   },
   "source": [
    "volume_position = torch.load(\"../data/run_11/volume_cell_position.pt\")\n",
    "print(f\"{int(len(volume_position) / 1000)}M volume points in 3D space\")\n",
    "print(f\"volume_position.shape: {volume_position.shape}\")\n",
    "# clamp volume positions for nicer visualization\n",
    "volume_position_clamp = volume_position.clamp(torch.tensor([-1, -3, -1.5]), torch.tensor([4, 3, 2]))\n",
    "plot_pointcloud_single(\n",
    "    volume_position_clamp,\n",
    "    title=\"volume positions\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=10000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH064MGeY_xN"
   },
   "source": [
    "We scale positions to have approximately range [0, 1000]. To do this, we use the min/max of the volume positions. For simplicity, we'll round them to [-40, 80]. You will see these values appear later when we normalize the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "cKK6DsbZZpfK",
    "outputId": "df649594-f7b5-415d-8dc1-18a990cb1f8e"
   },
   "source": [
    "volume_totalpcoeff = torch.load(\"../data/run_11/volume_cell_totalpcoeff.pt\")\n",
    "print(f\"{int(len(volume_totalpcoeff) / 1000)}M scalar volume pressure values\")\n",
    "print(f\"volume_totalpcoeff.shape: {volume_totalpcoeff.shape}\")\n",
    "plot_pointcloud_single(\n",
    "    volume_position_clamp,\n",
    "    # clamp for nicer visualization (original range is roughly [-1.7, 1.1])\n",
    "    color=volume_totalpcoeff.clamp(-0.5, 1.5),\n",
    "    title=\"volume positions\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=10000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "KfXdR3b-ZtGt",
    "outputId": "39af5eab-80d5-4ce5-ceec-54a79669d685"
   },
   "source": [
    "volume_velocity = torch.load(\"../data/run_11/volume_cell_velocity.pt\")\n",
    "print(f\"{int(len(volume_totalpcoeff) / 1000)}M 3D velocity vectors\")\n",
    "print(f\"volume_velocity.shape: {volume_velocity.shape}\")\n",
    "plot_pointcloud_single(\n",
    "    volume_position_clamp,\n",
    "    # clamp for nicer visualization (original range is roughly [0, 70])\n",
    "    color=volume_velocity.norm(dim=1).clamp(10, 50),\n",
    "    title=\"volume velocity\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=10000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "8QMCj8PcZu9v",
    "outputId": "073a3bd0-353d-4986-c536-d36bb05923e6"
   },
   "source": [
    "volume_vorticity = torch.load(\"../data/run_11/volume_cell_vorticity.pt\")\n",
    "print(f\"volume_vorticity.shape: {volume_vorticity.shape}\")\n",
    "# we convert vorticity to logspace because the value range is so large (roughly 0 to 250000)\n",
    "volume_vorticity_logscale = torch.sign(volume_vorticity) * torch.log1p(volume_vorticity.abs())\n",
    "plot_pointcloud_single(\n",
    "    volume_position_clamp,\n",
    "    # clamp for nicer visualization (original range is roughly [0, 18])\n",
    "    color=volume_vorticity_logscale.norm(dim=1).clamp(0, 12),\n",
    "    title=\"volume vorticity\",\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=10000,\n",
    "    figsize=(6, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3JT0RK0GdcI"
   },
   "source": [
    "# Preparing data for training/inference with AB-UPT\n",
    "\n",
    "AB-UPT requires anchor tokens, which are randomly sampled points in the surface/volume.\n",
    "\n",
    "We show how to do this on the example of surface points, but it is equivalent on volume points. So we randomly permute the surface points and split them into anchors and queries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xr2JHViXGcUi",
    "outputId": "c634d956-0fde-4095-e5a6-4da969ba9971"
   },
   "source": [
    "num_surface_anchors = 16384\n",
    "surface_perm = torch.randperm(len(surface_position))\n",
    "surface_anchor_idxs = surface_perm[:num_surface_anchors]\n",
    "surface_query_idxs = surface_perm[num_surface_anchors:]\n",
    "print(f\"num_surface_anchors: {len(surface_anchor_idxs)}\")\n",
    "print(f\"num_surface_queries: {len(surface_query_idxs)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34DGj__rHG8D"
   },
   "source": [
    "You can already see that anchors are a tiny fraction of the total points.\n",
    "\n",
    "Now we split all the surface variables (positions, pressure, wallshearstress) into anchors and queries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYkmlHk1HV0E",
    "outputId": "8161ebde-750b-4e95-ed62-ffc02fcb5436"
   },
   "source": [
    "# select anchors\n",
    "surface_anchor_position = surface_position[surface_anchor_idxs]\n",
    "surface_anchor_pressure = surface_pressure[surface_anchor_idxs]\n",
    "surface_anchor_wallshearstress = wallshearstress[surface_anchor_idxs]\n",
    "# select queries\n",
    "surface_query_position = surface_position[surface_query_idxs]\n",
    "surface_query_pressure = surface_pressure[surface_query_idxs]\n",
    "surface_query_wallshearstress = wallshearstress[surface_query_idxs]\n",
    "# print shapes\n",
    "print(f\"surface_anchor_position.shape: {surface_anchor_position.shape}\")\n",
    "print(f\"surface_anchor_pressure.shape: {surface_anchor_pressure.shape}\")\n",
    "print(f\"surface_anchor_wallshearstress.shape: {surface_anchor_wallshearstress.shape}\")\n",
    "print(f\"surface_query_position.shape: {surface_query_position.shape}\")\n",
    "print(f\"surface_query_pressure.shape: {surface_query_pressure.shape}\")\n",
    "print(f\"surface_query_wallshearstress.shape: {surface_query_wallshearstress.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJQTAqmkHv_1"
   },
   "source": [
    "For the geometry branch, we need a geometry representation and select some supernodes to aggregate local information. As geometry representation we can choose to either use the geometry mesh from the STL file or the surface cells from the CFD mesh. We use the CFD mesh here, which is stored in the `.vtp` file of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5iA8WBOHrH4",
    "outputId": "4521777b-6488-4fdd-f989-71e74b657c62"
   },
   "source": [
    "# use CFD mesh for geometry representation\n",
    "geometry_position = vtp.cell_centers().points\n",
    "print(f\"geometry_position.shape: {geometry_position.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMJOmfFaIdRn"
   },
   "source": [
    "These are also a lot of points, so we again subsample them because we dont need this extremely fine-grained representation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npQOGktkIb4m",
    "outputId": "c11fedd1-c0ac-4e06-a4e4-c50a5823c108"
   },
   "source": [
    "num_geometry_points = 65536\n",
    "geometry_perm = torch.randperm(len(geometry_position))\n",
    "geometry_idxs = geometry_perm[:num_geometry_points]\n",
    "print(f\"geometry_idxs.shape: {geometry_idxs.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaRmdutII1j7"
   },
   "source": [
    "As input to the geometry branch, we only need positions and additionally select some supernodes, which are randomly sampled from the geometry positions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oDDtXNqI07Q",
    "outputId": "3de8c24b-d62d-4688-f1e4-f09ed8f26c0d"
   },
   "source": [
    "# select positions\n",
    "geometry_position = torch.from_numpy(geometry_position[geometry_idxs])\n",
    "\n",
    "# select supernodes\n",
    "num_geometry_supernodes = 16384\n",
    "supernode_idxs = torch.randperm(num_geometry_points)\n",
    "print(f\"supernode_idxs.shape: {supernode_idxs.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GJ61wrKaasn"
   },
   "source": [
    "# Inference with AB-UPT\n",
    "\n",
    "Now we have everything necessary to run inference with AB-UPT:\n",
    "- Geometry positions (+ supernodes)\n",
    "- Surface positions\n",
    "- Volume positions\n",
    "\n",
    "We don't need anything else. However, the full forward pass and pre-processing is a bit more complicated (repeat the same process for volume, handle data normalization, add batch dimension, create auxiliary tensors for torch_geometrics, ...), so we'll use some pre-made components that:\n",
    "- load the data from the tensors that we created in the \"Inspect data\" section (DrivAerMLDataset class)\n",
    "- prepare surface, volume and geometry branch inputs with various preprocessings and normalizations. This is implemented in the AbuptCollator\" class, which is an advanced version of \"Preparing data for AB-UPT\" section.\n",
    "- Run inference to predict all 7M surface points (AnchoredBranchedUPT class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvxN4aQFbRp2"
   },
   "source": [
    "## Dataloading\n",
    "\n",
    "The dataset is responsible for loading the raw data from the disk into memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BzNMsEh4bRSa"
   },
   "source": [
    "# the dataset loads a raw (i.e., unprocessed sample)\n",
    "dataset = DrivAerMLDataset(root=\"../data\", split=\"test\")\n",
    "raw_sample = dict(\n",
    "    surface_position_vtp=dataset.getitem_surface_position_vtp(0),\n",
    "    surface_pressure=dataset.getitem_surface_pressure(0),\n",
    "    surface_wallshearstress=dataset.getitem_surface_wallshearstress(0),\n",
    "    volume_position=dataset.getitem_volume_position(0),\n",
    "    volume_totalpcoeff=dataset.getitem_volume_totalpcoeff(0),\n",
    "    volume_velocity=dataset.getitem_volume_velocity(0),\n",
    "    volume_vorticity=dataset.getitem_volume_vorticity(0),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gm7tfFoLcU38",
    "outputId": "de8379e8-45d3-4848-ad24-ec237cbeb72f"
   },
   "source": [
    "for key, value in raw_sample.items():\n",
    "  print(f\"{key}: {value.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_JNA04vcaaz"
   },
   "source": [
    "The collator then preprocesses the data:\n",
    "- subsample points if not the full resolution is needed\n",
    "- split into anchor/query points\n",
    "- normalize positions (to range [0, 1000])\n",
    "- normalize variables (to approximately mean=0 and std=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3djHLD6Ncrjr"
   },
   "source": [
    "collator = AbuptCollator(\n",
    "    num_geometry_points=65536,\n",
    "    num_surface_anchor_points=16384,\n",
    "    num_volume_anchor_points=16384,\n",
    "    num_geometry_supernodes=16384,\n",
    "    use_query_positions=True,\n",
    "    dataset=dataset,\n",
    ")\n",
    "# convert a list of samples to a preprocessed batch\n",
    "batch = collator([raw_sample])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCkYb06adDOe",
    "outputId": "979b3b34-dd67-4c1e-a042-b7ad3555e136"
   },
   "source": [
    "for key, value in batch.items():\n",
    "  print(f\"{key}: {value.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-UTrMrkKUay"
   },
   "source": [
    "## Forwad pass"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T14:50:44.260865Z",
     "start_time": "2025-06-26T14:50:42.903872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the checkpoint from HuggingFace Hub\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"EmmiAI/AB-UPT\"\n",
    "\n",
    "local_dir = \"./checkpoints\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    local_dir=local_dir,\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84c0be5dfd7f4e3b88c5e32e5cfd8dbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ab-upt-drivaerml-tutorial.th:   0%|          | 0.00/35.1M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77dc973be9cb42cb9f8a8ff89860635a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/pk/repos/anchored-branched-universal-physics-transformers/checkpoints'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIFV_YV5sgs2",
    "outputId": "2d86a83b-c0cd-40c3-91df-b9a958171d80"
   },
   "source": [
    "# load model from checkpoint\n",
    "abupt = AnchoredBranchedUPT().to(\"cuda\").eval()\n",
    "checkpoint = torch.load(\"./checkpoints/ab-upt-drivaerml-tutorial.th\", map_location=\"cuda\", weights_only=True)\n",
    "abupt.load_state_dict(checkpoint[\"state_dict\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQIvBWDgd4tv"
   },
   "source": [
    "# move batch to gpu\n",
    "batch = {key: value.to(\"cuda\") for key, value in batch.items()}\n",
    "\n",
    "# extract target variables for anchor\n",
    "target_surface_anchor_pressure = batch.pop(\"surface_anchor_pressure\")\n",
    "target_surface_anchor_wallshearstress = batch.pop(\"surface_anchor_wallshearstress\")\n",
    "target_volume_anchor_totalpcoeff = batch.pop(\"volume_anchor_totalpcoeff\")\n",
    "target_volume_anchor_velocity = batch.pop(\"volume_anchor_velocity\")\n",
    "target_volume_anchor_vorticity = batch.pop(\"volume_anchor_vorticity\")\n",
    "# extract target variables for queries\n",
    "target_surface_query_pressure = batch.pop(\"surface_query_pressure\")\n",
    "target_surface_query_wallshearstress = batch.pop(\"surface_query_wallshearstress\")\n",
    "target_volume_query_totalpcoeff = batch.pop(\"volume_query_totalpcoeff\")\n",
    "target_volume_query_velocity = batch.pop(\"volume_query_velocity\")\n",
    "target_volume_query_vorticity = batch.pop(\"volume_query_vorticity\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Wzgqzydlgyj"
   },
   "source": [
    "# we dont need all 8M surface points for now, so we use a subset to make this section run fast\n",
    "# later on, we will use all 8M points, dont worry :)\n",
    "num_surface_queries = 16384\n",
    "batch[\"surface_query_position\"] = batch[\"surface_query_position\"][:, :num_surface_queries]\n",
    "target_surface_query_pressure = target_surface_query_pressure[:num_surface_queries]\n",
    "target_surface_query_wallshearstress = target_surface_query_wallshearstress[:num_surface_queries]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for key, value in batch.items():\n",
    "  print(f\"{key}: {value.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkAQeiinMqXg",
    "outputId": "1ea3c6e6-558f-46be-a6e8-17c7c694454f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0a7DA5Z_eeTQ"
   },
   "source": [
    "# inference forward passes\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16), torch.no_grad():\n",
    "  prediction = abupt(**batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hvtSNxpZe3u8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "833435f0-7e79-4223-b959-62f4b0a7f9f1"
   },
   "source": [
    "for key, value in prediction.items():\n",
    "  print(f\"{key}: {value.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7enJ5dapK46D"
   },
   "source": [
    "## Visualizations\n",
    "\n",
    "Now lets visualize our predictions (we visualize them in normalized space for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Surface pressure"
   ],
   "metadata": {
    "id": "paLVSMYFO2RD"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IHnDhlYEK6uf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "outputId": "fcd435db-a81d-4324-b0ba-dfe0c12229a5"
   },
   "source": [
    "surface_query_positions_plot = batch[\"surface_query_position\"].cpu().squeeze(0)\n",
    "plot_pointcloud_double(\n",
    "    [surface_query_positions_plot, surface_query_positions_plot],\n",
    "    color=[target_surface_query_pressure.cpu().clamp(-2, 2), prediction[\"surface_query_pressure\"].cpu().clamp(-2, 2)],\n",
    "    delta_clamp=(-0.25, 0.25),\n",
    "    title=[\"target pressure\", \"predicted pressure\"],\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=2000,\n",
    "    figsize=(18, 6),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Volume x velocity"
   ],
   "metadata": {
    "id": "yntWPQnXO55j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "volume_query_positions_plot = batch[\"volume_query_position\"].cpu().squeeze(0)\n",
    "# clamp positions to see car better\n",
    "volume_query_positions_plot = volume_query_positions_plot.clamp(\n",
    "    torch.tensor([325, 308, 320]),\n",
    "    torch.tensor([366, 358, 350]),\n",
    ")\n",
    "plot_pointcloud_double(\n",
    "    [volume_query_positions_plot, volume_query_positions_plot],\n",
    "    color=[target_volume_query_velocity.cpu()[:, 0].clamp(-2, 2), prediction[\"volume_query_velocity\"].cpu()[:, 0].clamp(-2, 2)],\n",
    "    delta_clamp=(-0.25, 0.25),\n",
    "    title=[\"target velocity\", \"predicted velocity\"],\n",
    "    # increas this for more fidelity/larger plot\n",
    "    num_points=2000,\n",
    "    figsize=(18, 6),\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "e2JmE61AOx6a",
    "outputId": "17a3fb95-d881-4464-e558-65674bc866cb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik8EV6e_tk6g"
   },
   "source": [
    "## Error metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean squared error (MSE) loss in normalized space"
   ],
   "metadata": {
    "id": "pgfp68hxI1rb"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AqOXTTVUfOsA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d364466b-e70f-43b2-9c84-c6585b1c5d6c"
   },
   "source": [
    "surface_query_pressure_mse = nn.functional.mse_loss(prediction[\"surface_query_pressure\"], target_surface_query_pressure)\n",
    "print(surface_query_pressure_mse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "L2 error in denormalized space"
   ],
   "metadata": {
    "id": "oLuTUDkWI0RK"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJfw6e75ubNb",
    "outputId": "95596682-107d-4f63-cb26-d281bbad9f5a"
   },
   "source": [
    "# denormalize\n",
    "pressure_normalizer = collator.get_preprocessor(lambda c: isinstance(c, MomentNormalizationPreprocessor) and c.items == {\"surface_pressure\"})\n",
    "target_surface_query_pressure_denorm = pressure_normalizer.denormalize(target_surface_query_pressure)\n",
    "pres_surface_query_pressure_denorm = pressure_normalizer.denormalize(prediction[\"surface_query_pressure\"])\n",
    "\n",
    "# L2 error\n",
    "delta = target_surface_query_pressure_denorm - pres_surface_query_pressure_denorm\n",
    "l2_error = delta.norm() / target_surface_query_pressure_denorm.norm()\n",
    "print(l2_error)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY91R2gGwmm_"
   },
   "source": [
    "Note that the error varies between individual test samples and what points are sampled; on average it is roughly 4%"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drag & lift coefficients\n",
    "\n",
    "To calculate drag and lift coefficients, we need some more data such as simulation specific scalars (rho, v_inf, a reference area) and cell specific data (surface normal and surface area)."
   ],
   "metadata": {
    "id": "kJ5ETwSg_ySu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define and load reference values\n",
    "rho = 1.0\n",
    "v_inf = 38.889\n",
    "inlet_direction = torch.tensor([1, 0, 0], dtype=torch.float)\n",
    "lift_direction = torch.tensor([0, 0, 1], dtype=torch.float)\n",
    "reference_area = torch.tensor(pd.read_csv(\"../data/run_11/geo_ref_11.csv\")['aRef'][0])\n",
    "\n",
    "# extract surface normals and areas\n",
    "surface_normal = torch.from_numpy(vtp.cell_normals)\n",
    "surface_area = torch.from_numpy(vtp.compute_cell_sizes(length=False, volume=False)[\"Area\"]).float()"
   ],
   "metadata": {
    "id": "SzzLAwst_0_U"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can easily calculate the drag and lift coefficients from the HRLES CFD simulations."
   ],
   "metadata": {
    "id": "zDAaOy1UEe-4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# calculate forces of the HRLES CFD simulation\n",
    "target_pforce = (surface_normal.T * (surface_pressure * surface_area)).sum(dim=1)\n",
    "target_sforce = -(wallshearstress.T * surface_area).sum(dim=1)\n",
    "target_total_force = target_pforce + target_sforce\n",
    "\n",
    "# calculate coefficients of the HRLES CFD simulation\n",
    "target_cd = torch.dot(target_total_force, inlet_direction) / (0.5 * rho * v_inf**2 * reference_area)\n",
    "target_cl = torch.dot(target_total_force, lift_direction) / (0.5 * rho * v_inf**2 * reference_area)\n",
    "\n",
    "print(f\"HRLES drag coefficient: {target_cd:.4f}\")\n",
    "print(f\"HRLES lift coefficient: {target_cl:.4f}\")"
   ],
   "metadata": {
    "id": "aFpLEuYkDHWy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99d8d1eb-b0c7-455e-c317-7aaafbcc88de"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to predict these coefficients with AB-UPT. To do this, we need to decode the full 8M points of the surface mesh to obtain corresponding pressure and wallshearstress values.\n",
    "\n",
    "We use the data that was previously loaded and simply plug in the full 9M surface mesh positions as surface queries."
   ],
   "metadata": {
    "id": "LM1_b709EJEq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# reload the batch because we modified it before\n",
    "batch = collator.preprocess_inputs_only([raw_sample])\n",
    "# move to gpu\n",
    "batch = {key: value.to(\"cuda\") for key, value in batch.items()}\n",
    "\n",
    "# we dont need the volume query positions from the previously loaded batch, so we use only 1 query position to make inference faster\n",
    "batch[\"volume_query_position\"] = batch[\"volume_query_position\"][:, :1]\n",
    "# we dont use the surface position from the batch as they are downsampled\n",
    "batch.pop(\"surface_query_position\")\n",
    "\n",
    "# normalize surface positions\n",
    "position_normalizer = collator.get_preprocessor(lambda c: isinstance(c, PositionNormalizationPreprocessor))\n",
    "surface_position = position_normalizer(\n",
    "    [\n",
    "        dict(\n",
    "            surface_position_vtp=surface_position,\n",
    "            volume_position=torch.randn(1, 3),\n",
    "        )\n",
    "    ],\n",
    ")[0][\"surface_position_vtp\"]"
   ],
   "metadata": {
    "id": "FRg8ZV-kE11j"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# full inference forward pass (note that this propagates ~8M points, so it can take a while on the free colab GPUs)\n",
    "# 8M points dont fit into GPU meomry, so we need to chunk it\n",
    "predictions = []\n",
    "for surface_query_position_chunk in tqdm(surface_position.chunk(chunks=10)):\n",
    "  with torch.autocast(device_type=\"cuda\", dtype=torch.float16), torch.no_grad():\n",
    "    predictions.append(abupt(surface_query_position=surface_query_position_chunk.to(\"cuda\").unsqueeze(0), **batch))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpPQNqE0fnO6",
    "outputId": "91a9b66e-9b73-4600-ae00-79a04ee1a574"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# concatenate chunks\n",
    "prediction = {}\n",
    "for key in predictions[0].keys():\n",
    "  if \"surface_query\" in key:\n",
    "    # concat surface_query predictions\n",
    "    prediction[key] = torch.cat([predictions[i][key] for i in range(len(predictions))])"
   ],
   "metadata": {
    "id": "fzRgRC7KRhJL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# denormalize predictions\n",
    "wallshearstress_normalizer = collator.get_preprocessor(lambda c: isinstance(c, MomentNormalizationPreprocessor) and c.items == {\"surface_wallshearstress\"})\n",
    "surface_query_pressure_denorm = pressure_normalizer.denormalize(prediction[\"surface_query_pressure\"])\n",
    "surface_query_wallshearstress_denorm = wallshearstress_normalizer.denormalize(prediction[\"surface_query_wallshearstress\"])"
   ],
   "metadata": {
    "id": "kUL02ncOTjSH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we simply replace the surface pressure and the wallshearstress with the model predictions in the coefficient formulars."
   ],
   "metadata": {
    "id": "r4ave4UYFmdz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# calculate predicted forces\n",
    "pred_pforce = (surface_normal.T * (surface_query_pressure_denorm.squeeze(-1).cpu() * surface_area)).sum(dim=1)\n",
    "pred_sforce = -(surface_query_wallshearstress_denorm.cpu().T * surface_area).sum(dim=1)\n",
    "pred_total_force = pred_pforce + pred_sforce\n",
    "# calculate predicted coefficients\n",
    "pred_cd = torch.dot(pred_total_force, inlet_direction) / (0.5 * rho * v_inf**2 * reference_area)\n",
    "pred_cl = torch.dot(pred_total_force, lift_direction) / (0.5 * rho * v_inf**2 * reference_area)\n",
    "\n",
    "print(f\"AB-UPT drag coefficient: {pred_cd:.4f}\")\n",
    "print(f\"AB-UPT lift coefficient: {pred_cl:.4f}\")"
   ],
   "metadata": {
    "id": "s5C_NyKwC7e1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0b705c85-bdf7-4756-8c44-63807d4f02d5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's pretty close to the actual value"
   ],
   "metadata": {
    "id": "J3Uia6-IUXcE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"HRLES drag coefficient: {target_cd:.4f}\")\n",
    "print(f\"AB-UPT drag coefficient: {pred_cd:.4f}\")\n",
    "print(f\"Absolute error {(target_cd - pred_cd).abs().item():.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9U9QP3zUamX",
    "outputId": "c5ff835f-0e7b-4746-922d-450ef96f77b5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"HRLES lift coefficient: {target_cl:.4f}\")\n",
    "print(f\"AB-UPT lift coefficient: {pred_cl:.4f}\")\n",
    "print(f\"Absolute error {(target_cl - pred_cl).abs().item():.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeMsyRXDUv_s",
    "outputId": "fffd3540-a6f8-4555-9773-60095766749a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WS7bpRdNXUb"
   },
   "source": [
    "## Streamline visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now plot the streamlines around the car, based on the velocity predictions. Next to that, we plot the groun truth first, to have a side-by-side comparison."
   ],
   "metadata": {
    "id": "LgR5mZtfmUhk"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w27jQvZ1NXUb"
   },
   "source": [
    "# load ground truth data\n",
    "surface_position = dataset.getitem_surface_position_vtp(0)\n",
    "volume_position = dataset.getitem_volume_position(0)\n",
    "volume_velocity = dataset.getitem_volume_velocity(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wezLF4_SNXUb"
   },
   "source": [
    "Predict the volume velocity with AB-UPT"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# reload the batch because we previously modified it\n",
    "batch = collator.preprocess_inputs_only([raw_sample])\n",
    "# remove surface queries (use only a single surface position query)\n",
    "batch[\"surface_query_position\"] = batch[\"surface_query_position\"][:, :1]\n",
    "# move batch to device\n",
    "batch = {key: value.to(\"cuda\") for key, value in batch.items()}\n",
    "# create volume predictions\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16), torch.no_grad():\n",
    "  prediction = abupt(**batch)"
   ],
   "metadata": {
    "id": "XE05KiP9WJXn"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iy4mz0n6NXUb"
   },
   "source": [
    "# denormalize input positions and velocity predictions\n",
    "velocity_normalizer = collator.get_preprocessor(lambda c: isinstance(c, MomentNormalizationPreprocessor) and c.items == {\"volume_velocity\"})\n",
    "position_normalizer = collator.get_preprocessor(lambda c: isinstance(c, PositionNormalizationPreprocessor))\n",
    "volume_position_query = position_normalizer.denormalize(value=batch[\"volume_query_position\"])[0]\n",
    "volume_velocity_pred = velocity_normalizer.denormalize(value=prediction[\"volume_query_velocity\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_streamlines(\n",
    "    surface_position=surface_position,\n",
    "    volume_position_gt=volume_position,\n",
    "    volume_velocity_gt=volume_velocity,\n",
    "    volume_position_pred=volume_position_query.cpu(),\n",
    "    volume_velocity_pred=volume_velocity_pred.cpu(),\n",
    ")"
   ],
   "metadata": {
    "id": "EpgEDRazbZIL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "outputId": "302769eb-a84b-4e8b-d0f5-421b3598a79d"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
